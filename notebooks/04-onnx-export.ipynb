{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBasof3bv1LB"
   },
   "source": [
    "<h1><center>How to export ðŸ¤— Transformers Models to ONNX ?<h1><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ONNX](http://onnx.ai/) is open format graph representation for machine learning models. It allows to save your neural network's computation graph in a framework agnostic way, which might be particulary helpful when deploying deep learning models.\n",
    "\n",
    "Indeed, businesses might have other requirements _(languages, hardware, ...)_ for which the training framework might not be the best suited in inference scenarios. In that context, having a representation of the actual computation graph that can be shared accross various business units and logics across an organization might be a desirable component.\n",
    "\n",
    "Along with the serialization format, ONNX also provides a runtime library which allows efficient and hardware specific execution of the ONNX graph. This is done through the [onnxruntime](https://microsoft.github.io/onnxruntime/) project and already includes collaborations with many hardware vendors to seamlessly deploy models on various platforms.\n",
    "\n",
    "Through this notebook we'll walk you through the process to convert a PyTorch or TensorFlow transformers model to the [ONNX](http://onnx.ai/) and leverage [onnxruntime](https://microsoft.github.io/onnxruntime/) to run inference tasks on models from  ðŸ¤— __transformers__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNnbrSg-5e1s"
   },
   "source": [
    "## Exporting ðŸ¤— transformers model to ONNX\n",
    "\n",
    "---\n",
    "\n",
    "Exporting models _(either PyTorch or TensorFlow)_ is easily achieved through the conversion tool provided as part of ðŸ¤— __transformers__ repository. \n",
    "\n",
    "Under the hood the process is sensibly the following: \n",
    "\n",
    "1. Allocate the model from transformers (PyTorch or TensorFlow)\n",
    "2. Forward dummy inputs through the model this way ONNX can record the set of operations executed\n",
    "3. Optionally define dynamic axes on inputs and output tensors\n",
    "4. Save the graph along with the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers@onnx-export\n",
    "!pip install pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwAaOchY4N2-"
   },
   "outputs": [],
   "source": [
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "# Handles all the above steps for you\n",
    "convert(framework=\"pt\", model=\"bert-base-cased\", output=\"onnx/bert-base-cased.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjpN15-F_W-6"
   },
   "source": [
    "Here is the ONNX generated computation graph visualized. You can see a very fine grained computational graph. This is what ONNX call the \"raw\" graph which\n",
    "is expressed only through ONNX opcode, available into all providers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to leverage runtime for inference over an ONNX graph\n",
    "\n",
    "---\n",
    "\n",
    "As mentionned in the introduction, ONNX is serialization format and many side projects can load the saved graph and run the actual computation on it. Here, we'll focus on the official [onnxruntime](https://microsoft.github.io/onnxruntime/). The runtime is implemented as in C++ for performance reasons and provides API/Bindings for C++, C, C#, Java and Python.\n",
    "\n",
    "Next, we will use the Python API to highlight how to load a serialized ONNX graph and run inference workload on various backends.\n",
    "\n",
    "**onnxruntime** is available through pip:\n",
    "\n",
    "- onnxruntime: ONNX + MLAS (Microsoft Linear Algebra Subprograms)\n",
    "- onnxruntime-gpu: ONNX + MLAS + CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers onnxruntime-gpu onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from onnxruntime import GraphOptimizationLevel, ExecutionMode, InferenceSession, SessionOptions, get_all_providers\n",
    "from transformers import BertTokenizerFast\n",
    "from psutil import cpu_count\n",
    "\n",
    "# Constants from the performance optimization available in onnxruntime\n",
    "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\n",
    "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-gP08tHfBvgY"
   },
   "source": [
    "## Preparing for an Inference Session\n",
    "\n",
    "---\n",
    "\n",
    "Inference is done using a specific backend definition which turns on hardware specific optimizations of the graph. \n",
    "\n",
    "Optimizations are basically of three kinds: \n",
    "\n",
    "- **Constant Folding**: Convert static variables to constants in the graph \n",
    "- **Deadcode Elimination**: Remove nodes never accessed in the graph\n",
    "- **Operator Fusing**: Merge multiple instruction into one (Linear -> ReLU can be fused to be LinearReLU)\n",
    "\n",
    "All of this is done through ONNX by settings specific SessionOptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2k-jHLfdcTFS"
   },
   "outputs": [],
   "source": [
    "def create_model_for_provider(model_path: str, provider: str) -> InferenceSession: \n",
    "  \n",
    "  assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
    "\n",
    "  # Few properties than might have an impact on performances (provided by MS)\n",
    "  options = SessionOptions()\n",
    "  options.intra_op_num_threads = 1\n",
    "  options.execution_mode = ExecutionMode.ORT_SEQUENTIAL\n",
    "\n",
    "  # This line tells CPU Backend to store the optimized graph for this backend\n",
    "  options.optimized_model_filepath = f\"onnx/bert-base-cased-{provider.split('Execution')[0]}-optimized.onnx\"\n",
    "\n",
    "  # Load the model as a graph and prepare the CPU backend \n",
    "  return InferenceSession(model_path, options, providers=[provider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "teJdG3amE-hR"
   },
   "source": [
    "## Forwarding through our optimized ONNX model running on CPU\n",
    "\n",
    "---\n",
    "\n",
    "When the model is loaded for inference over a specific provider, for instance **CPUExecutionProvider** as above, an optimized graph can be saved. This graph will feature various optimizations, and you might be able to see some **higher-level** operations in the graph such as:\n",
    "- EmbedLayerNormalization\n",
    "- Attention\n",
    "- FastGeLU\n",
    "\n",
    "These operations are gathering multiple operations into one bigger (fusing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dmC22kJfVGYe",
    "outputId": "f3aba5dc-15c0-4f82-b38c-1bbae1bf112e"
   },
   "outputs": [
    {
     "ename": "RuntimeException",
     "evalue": "[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: D:\\Workspace\\Python\\onnxruntime\\onnxruntime\\core\\optimizer\\optimizer_execution_frame.cc:61 onnxruntime::OptimizerExecutionFrame::Info::Info [ONNXRuntimeError] : 1 : FAIL : GetFileSizeEx encoder.layer.11.output.dense.weight fail, errcode = 6\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f6896abec728>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-cased\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcpu_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model_for_provider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"onnx/bert-base-cased.onnx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"CPUExecutionProvider\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Inputs are provided through numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"My name is Bert\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-9d384718fee9>\u001b[0m in \u001b[0;36mcreate_model_for_provider\u001b[1;34m(model_path, provider)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m   \u001b[1;31m# Load the model as a graph and prepare the CPU backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mInferenceSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproviders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprovider\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Workspace\\Miniconda3\\envs\\huggingface\\lib\\site-packages\\onnxruntime\\capi\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_bytes, sess_options, providers)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path_or_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_or_bytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Workspace\\Miniconda3\\envs\\huggingface\\lib\\site-packages\\onnxruntime\\capi\\session.py\u001b[0m in \u001b[0;36m_load_model\u001b[1;34m(self, providers)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unable to load from type '{0}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path_or_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeException\u001b[0m: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: D:\\Workspace\\Python\\onnxruntime\\onnxruntime\\core\\optimizer\\optimizer_execution_frame.cc:61 onnxruntime::OptimizerExecutionFrame::Info::Info [ONNXRuntimeError] : 1 : FAIL : GetFileSizeEx encoder.layer.11.output.dense.weight fail, errcode = 6\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "cpu_model = create_model_for_provider(\"onnx/bert-base-cased.onnx\", \"CPUExecutionProvider\")\n",
    "\n",
    "# Inputs are provided through numpy array\n",
    "model_inputs = tokenizer.encode_plus(\"My name is Bert\")\n",
    "inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()}\n",
    "\n",
    "# Run the model (None = get all the outputs)\n",
    "sequence, pooled = cpu_model.run(None, inputs_onnx)\n",
    "\n",
    "# Print information about outputs\n",
    "\n",
    "print(f\"Sequence output: {sequence.shape}, Pooled output: {pooled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kda1e7TkEqNR"
   },
   "source": [
    "## Benchmarking different CPU & GPU providers\n",
    "\n",
    "_**Disclamer: results may vary from the actual hardware used to run the model**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "WcdFZCvImVig",
    "outputId": "bfd779a1-0bc7-42db-8587-e52a485ec5e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing GPU inference on Tesla P100-PCIE-16GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 199.60it/s]\n",
      "Tracking inference time on CUDAExecutionProvider: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 286.78it/s]\n",
      "Warming up: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  9.87it/s]\n",
      "Tracking inference time on CPUExecutionProvider: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09<00:00, 10.56it/s]\n",
      "Warming up: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  9.46it/s]\n",
      "Tracking inference time on TensorrtExecutionProvider: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09<00:00, 10.23it/s]\n",
      "Warming up: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  9.49it/s]\n",
      "Tracking inference time on DnnlExecutionProvider: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09<00:00, 10.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from os.path import getsize\n",
    "from torch.cuda import get_device_name\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from time import time\n",
    "from tqdm import trange\n",
    "\n",
    "@contextmanager\n",
    "def track_infer_time(buffer: [int]):\n",
    "    start = time()\n",
    "    yield\n",
    "    end = time()\n",
    "\n",
    "    buffer.append(end - start)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OnnxInferenceResult:\n",
    "  model_size: int\n",
    "  model_inference_time: [int]  \n",
    "  optimized_model_path: str\n",
    "\n",
    "\n",
    "# All the providers we'll be using in the test\n",
    "results = {}\n",
    "providers = [\n",
    "  \"CUDAExecutionProvider\",\n",
    "  \"CPUExecutionProvider\",            \n",
    "  \"TensorrtExecutionProvider\",\n",
    "  \"DnnlExecutionProvider\",          \n",
    "]\n",
    "\n",
    "print(f\"Doing GPU inference on {get_device_name(0)}\")\n",
    "\n",
    "# Iterate over all the providers\n",
    "for provider in providers:\n",
    "\n",
    "  # Create the model with the specified provider\n",
    "  model = create_model_for_provider(provider)\n",
    "\n",
    "  # Keep track of the inference time\n",
    "  time_buffer = []\n",
    "\n",
    "  # Warm up the model\n",
    "  for _ in trange(10, desc=\"Warming up\"):\n",
    "    model.run(None, inputs_onnx)\n",
    "\n",
    "  # Compute \n",
    "  for _ in trange(100, desc=f\"Tracking inference time on {provider}\"):\n",
    "    with track_infer_time(time_buffer):\n",
    "      model.run(None, inputs_onnx)\n",
    "\n",
    "  # Store the result\n",
    "  results[provider] = OnnxInferenceResult(\n",
    "      os.path.getsize(model.get_session_options().optimized_model_filepath), \n",
    "      time_buffer,\n",
    "      model.get_session_options().optimized_model_filepath\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PS_49goe197g",
    "outputId": "0ef0f70c-f5a7-46a0-949a-1a93f231d193"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  7.77it/s]\n",
      "Tracking inference time on PyTorch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:12<00:00,  7.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add Pytorch to the providers\n",
    "for _ in trange(10, desc=\"Warming up\"):\n",
    "  model_pt(**model_inputs)\n",
    "\n",
    "# Compute \n",
    "time_buffer = []\n",
    "for _ in trange(100, desc=f\"Tracking inference time on PyTorch\"):\n",
    "  with track_infer_time(time_buffer):\n",
    "    model_pt(**model_inputs)\n",
    "\n",
    "# Store the result\n",
    "results[\"Pytorch\"] = OnnxInferenceResult(\n",
    "    0, \n",
    "    time_buffer, \n",
    "    model.get_session_options().optimized_model_filepath\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "colab_type": "code",
    "id": "dj-rS8AcqRZQ",
    "outputId": "b4bf07d1-a7b4-4eff-e6bd-d5d424fd17fb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Compute average inference time + std\n",
    "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
    "time_results_std = np.std([v.model_inference_time for v in results.values()]) * 1000\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.ylabel = \"Avg Inference time (ms)\" \n",
    "plt.title = \"Average inference time (ms) for each provider\"\n",
    "plt.bar(time_results.keys(), time_results.values(), yerr=time_results_std)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ONNX Overview",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
